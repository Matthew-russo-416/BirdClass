# Filtering Bird Data
# Matthew Russo

# Here we play around with filtering the raw meta data a few ways. Included in the meta data are the latitude and
# longitude coordinates associated with each audio file. We split the data as follows:
#   - We use two unsupervised learning methods to split the data into groups based on their geographic location.
#     First we use K-means clustering to aggregate data into geographic clusters, which are then visualized on a world
#     map. The "elbow method" is used to determine an appropriate number of clusters.
#   - The second unsupervised learning method we try on the original raw
#     data is DBSCAN clustering. We similarly use this method to aggregate data into geographic clusters, which are
#     again visualized on a world map.
#   - The third method we use to filter the data is to first map the latitude-longitude
#     coordinate pairs to their corresponding countries, and then filter by country or countries. We demonstrate this by
#     filtering out data below an audio quality threshold and outside of the US, Canada, and Mexico.

import numpy as np
import pandas as pd
import geopandas as gpd
import reverse_geocode
import sys
import matplotlib.gridspec as gridspec

from matplotlib import pyplot as plt
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN

# Read in the training meta data from 'train_metadata.csv' into a pandas dataframe.
# Each row corresponds to an element in the training data set—i.e. an audio file.
train_meta = pd.read_csv('train_metadata.csv',
                         usecols=['primary_label', 'latitude', 'longitude',
                                  'common_name', 'date', 'filename',
                                  'rating'])

# Now we use the geographic data and geopandas to visualize the geographic distribution of the data.
# Create a geodataframe from the latitudes and longitudes associated with the training data
gdf = gpd.GeoDataFrame(
    train_meta, geometry=gpd.points_from_xy(train_meta.longitude, train_meta.latitude))

# Create a numpy array of latitude-longitude pairs associated with the training data.
X = np.column_stack((train_meta['latitude'].values, train_meta['longitude'].values))

# Import a map of the world for use in visualization of data distribution
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))


def vis_all_data():
    # Initiate the plot
    fig1, ax = plt.subplots()

    # Plot a blank world mao to overlay the data on
    world.plot(color='white', edgecolor='k', linewidth=2, ax=ax)

    # Plot the training data, where the color of the plotted points are determined by
    # primary label (bird type).
    gdf.plot(column='primary_label', categorical=True, markersize=1, ax=ax)
    plt.show()


################################
# Method 1: K-Means Clustering #
################################

def filt_by_Kmeans():
    # Here we perform K-means clustering using the geographic data.
    # We vary the number of clusters from 2 to 25 and use the "elbow method" to
    # determine an appropriate number of clusters. The "elbow method" essentially has
    # to do with diminishing returns.

    # Initiate the plot
    fig2 = plt.figure(tight_layout=True)
    gs = gridspec.GridSpec(1, 3)

    ax2a = fig2.add_subplot(gs[0])

    # Initiate ssd (sum of squared distances) and cluster_no
    ssd = []
    cluster_no = []

    # Perform K-means clustering for i=2 to i=25 clusters. We calulate the inertia
    # (i.e. sum of squared distance from centroid over all elements in a given cluster
    # and over all clusters) for each i and plot them against the number of clusters.
    for i in range(2, 25):
        km = KMeans(n_clusters=i, random_state=0)
        cluster_no.append(km.fit_predict(X))
        ssd.append(km.inertia_)

    # Plot inertias vs number of clusters
    ax2a.scatter(np.arange(2, 25), ssd[:])
    ax2a.set_title('Plot of Inertias')

    # Using the "Elbow Method": Looks like 7 or 8 is probably good.
    # Create a dataframe containing the clusters generated by K-means clustering with
    # 7 clusters.
    clusters = pd.DataFrame(cluster_no[6], columns=['cluster'])

    # Join the clusters and gdf (geodataframe—latitudes and longitudes) dataframes.
    gdf_c = gdf.join(clusters)

    # Plot blank world map and overlay clusters on it.
    ax2b = fig2.add_subplot(gs[1:])
    world.plot(color='white', edgecolor='k', linewidth=2, ax=ax2b)
    gdf_c.plot(column='cluster', cmap='Set2', markersize=1, ax=ax2b)
    ax2b.set_title('K-Means with with 7 clusters')
    plt.show()


####################
# Method 2: DBSCAN #
####################

def filt_by_DBSCAN():
    # Here we try the unsupervised learning method DBSCAN instead of K-Means clustering to separate the data into groups using geographic data.

    # Initiate DBSCAN with eps and min_samples and create clusters from the geodata. Here:
    # eps         = radius of neighborhood
    # min_samples = min number of points needed in an eps neighborhood of
    #               a point for the point to be considered a core point
    clusters_from_DBSCAN = DBSCAN(eps=4, min_samples=20).fit_predict(X)

    # Create dataframe from cluster data and join with geodataframe
    clusters_DBSCAN = pd.DataFrame(clusters_from_DBSCAN, columns=['cluster_DBSCAN'])
    gdf_cd = gdf.join(clusters_DBSCAN)

    # Initiate plot
    fig3, ax3 = plt.subplots()

    # Plot blank world map and overlay clusters on it.
    world.plot(color='white', edgecolor='k', linewidth=2, ax=ax3)
    gdf_cd.plot(column='cluster_DBSCAN', c=clusters_from_DBSCAN, cmap='Set2',
                markersize=1, ax=ax3)
    plt.show()


#############################################
# Method 3: Filter by Country, Date, Rating #
#############################################
def filt_by_CDR():
    # First we find the indices for the bird data from the US, Canada, or Mexico.
    # To do this, we get the physical addresses associated with the
    # latitude-longitude pairs in X
    X_useful_loc = reverse_geocode.search(X)

    # Create array with the country corresponding to each element in the training data
    X_countries = np.array([d['country'] for d in X_useful_loc])

    # North America Booleans
    USA = X_countries == 'United States'
    CAN = X_countries == 'Canada'
    MEX = X_countries == 'Mexico'
    NA = (USA | CAN | MEX)

    # Indices corresponding to data from the US, Canada, or Mexico
    X_NA_ind = np.where(NA)

    # Create dataframe from US, Canada, Mexico data
    train_meta_NA = train_meta.iloc[X_NA_ind[0]]

    # Now sort dates chronologically (for possible use later)
    train_meta_NA_date_sort = train_meta_NA.sort_values(by='date')

    # Convert valid dates to datetime and invalid to NAT
    pd.to_datetime(train_meta_NA_date_sort["date"], errors='coerce')

    # Date Range and audio rating threshold
    start_date = "1900-1-1"
    end_date = "2021-12-31"
    min_rating = 3.5  # threshold for audio rating

    # Create booleans for date range and audio rating
    after_start_date = train_meta_NA_date_sort["date"] >= start_date
    before_end_date = train_meta_NA_date_sort["date"] <= end_date
    between_two_dates = after_start_date & before_end_date
    above_min_rating = train_meta_NA_date_sort["rating"] >= min_rating

    # Filter out data not in date range and below min_rating
    train_meta_NA_good_date_rating = train_meta_NA_date_sort[between_two_dates & above_min_rating]

    # Now we visualize the filtered data on a map of North America. The color of
    # each point on the map corresponds to primary label (bird type) for the audio
    # file recorded at that location.
    # Create a geodataframe from the filtered data 'train_meta_NA_good_date_rating'.
    gdf_NA = gpd.GeoDataFrame(train_meta_NA_good_date_rating,
                              geometry=gpd.points_from_xy(train_meta_NA_good_date_rating.longitude,
                                                          train_meta_NA_good_date_rating.latitude))

    # Initialize plot
    fig4, (ax4a, ax4b) = plt.subplots(2, 1, constrained_layout=True)

    # Plot blank map of North America and overlay filtered data from 'gdf_NA' on it.
    NA_fig = world[world.continent == "North America"]
    NA_fig.plot(color='white', edgecolor='k', linewidth=2, ax=ax4a)
    gdf_NA.plot(column='primary_label', categorical=True, markersize=2, ax=ax4a)
    ax4a.set_title('North American Data')

    # Sort the filtered North American training set train_meta_NA_good_date_rating by
    # number of audio files per bird type from highest to lowest. Then pull the bird
    # names of the top num_bird_types.
    num_bird_types = 20
    with pd.option_context('display.max_rows', 1000, 'display.max_columns', None):
        names_to_use = train_meta_NA_good_date_rating['common_name'].value_counts()[:num_bird_types].index

    # Now we keep only the files corresponding to the birds in the names_to_use list.
    most_data_avail = train_meta_NA_good_date_rating['common_name'].isin(names_to_use.values)
    train_data_top_birds = train_meta_NA_good_date_rating[most_data_avail]

    # Create a geodataframe from the filtered data 'train_meta_NA_good_date_rating'.
    gdf_NA_top_birds = gpd.GeoDataFrame(train_data_top_birds,
                                        geometry=gpd.points_from_xy(train_data_top_birds.longitude,
                                                                    train_data_top_birds.latitude))

    # Plot blank map of North America and overlay filtered data from 'gdf_NA' on it.
    NA_fig.plot(color='white', edgecolor='k', linewidth=2, ax=ax4b)
    gdf_NA_top_birds.plot(column='primary_label', categorical=True, markersize=2, ax=ax4b)
    ax4b.set_title('Filtered North American Data')
    plt.show()


# This function allows us to choose which filtering option to visualize. The options are:
# - filt_opt = 0 -> original data: run vis_all_data()
# - filt_opt = 1 -> K-means: run filt_by_Kmeans()
# - filt_opt = 2 -> DBSCAN: run filt_by_DBSCAN()
# - filt_opt = 3 -> Country/Date/Rating: run filt_by_CDR

if __name__ == "__main__":
    filt_opt = int(sys.argv[1])
    if filt_opt == 0:
        vis_all_data()
    elif filt_opt == 1:
        filt_by_Kmeans()
    elif filt_opt == 2:
        filt_by_DBSCAN()
    elif filt_opt == 3:
        filt_by_Kmeans()
